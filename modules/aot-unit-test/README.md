# aot-unit-test AOT Inference Module

> 🔧 **Generated by HeliosAOT** on 2025-06-04

## ⚡️ Overview

This folder contains an ahead-of-time (AOT) compiled C inference module for **aot-unit-test**, targeting **neuralspot**.
It implements your LiteRT model **vww_96_int8.tflite**.

- [Model Contents](#module-contents)
- [Model Summary](#model-summary)
- [Configuration](#configuration)
- [Integration / Build](#integration--build)
- [Usage Example](#usage-example)
- [License](#license)
- [Model Diagram](#model-diagram)

## 📦 Module Contents

```console
aot-unit-test
├── LICENSE.txt ← license for this module
├── README.md ← this file
├── includes-api/ ← API header files
├── src/ ← C source files
├── zephyr/ ← For Zephyr integration
├── module.mk ← For neuralSPOT integration
└── CMakeLists.txt ← For Zephyr integration
```

## 🧠 Model Summary

| Property               | Value                         |
|------------------------|-------------------------------|
| **Model name**         | vww_96_int8.tflite                 |
| **Model Version**      | v1.0.0              |
| **Subgraph Index**     | 0                   |
| **Operator Count**     | 30            |
| **Unique Ops**         | 5           |
| **Input Shape**        | (  1, 96, 96, 3)        |
| **Output Shape**       | (  1, 2)       |

## ⚙️ Operators Used

| Operator           | Count |
|--------------------|-------|
| `CONV_2D` | 14 |
| `DEPTHWISE_CONV_2D` | 13 |
| `AVERAGE_POOL_2D` | 1 |
| `FULLY_CONNECTED` | 1 |
| `SOFTMAX` | 1 |

## 🎛️ Configuration Parameters

```yaml
model_path: /Users/adampage/Downloads/helios-aot-tflites/vww_96_int8.tflite
output_path: /Users/adampage/Ambiq/research/mve-kernel-exp/modules
module_name: aot-unit-test
module_type: neuralspot
prefix: aot
memory_planner: greedy
operator_attributes: []
transforms:
  transforms:
    PruneReshapeOps: true
    PruneSqueezeOps: true
include_test: true
test_tolerance: 1.0
subgraph: 0
model_version: v1.0.0
verbose: 2
log_file: null

```

## 🛠️ Integration / Build

### Zephyr RTOS

1. Copy this module into your Zephyr project under modules/lib/aot-unit-test.

2. Add to your west.yml:

    ```yaml

    projects:
    - name: aot-unit-test
        path: modules/lib/aot-unit-test
        revision: main   # or pin a tag/commit
    ```

3. In your application’s CMakeLists.txt:

    ```cmake
    find_package(aot-unit-test REQUIRED)
    target_link_libraries(app PRIVATE aot-unit-test::aot-unit-test)
    ```

4. Build with:

    ```bash
    west build -b <board> samples/your_app
    ```

### neuralSPOT

🚧 Coming soon...

## 🚀 Usage Example

```c

#include "aot_model.h"

// Define input/output buffer
int8_t input[aot_input_len] = { 0 };
int8_t output[aot_output_len];

// Define model context and callback
static void aot_model_operator_cb(
    int32_t op,
    aot_operator_state_e state,
    int32_t status,
    void *user_data
) {
    switch (state) {
        case aot_model_state_started:
            ns_lp_printf("Operation %d started\n", op);
            break;
        case aot_model_state_finished:
            ns_lp_printf("Operation %d finished with status %d\n", op, status);
            break;
    }
}


// This is optional, but useful for profiling or debugging
aot_model_context_t aot_model_ctx = {
    .callback = aot_model_operator_cb,
    .user_data = NULL
};

int main(void) {
    // Initialize model
    int32_t status;
    status = aot_model_init(&aot_model_ctx);

    // Run inference
    status = aot_model_run(&aot_model_ctx, input, output);

    return 0;
}

```

## License

This code is licensed under the terms in [LICENSE.txt](./LICENSE.txt).
Use of this generated module is restricted to Ambiq hardware only.

---

## Model Diagram

```mermaid
---
config:
  look: handDrawn
  theme: neutral
---
flowchart TB
  op0["0: conv_0"]
  op1["1: depthwise_conv_1"]
  op2["2: conv_2"]
  op3["3: depthwise_conv_3"]
  op4["4: conv_4"]
  op5["5: depthwise_conv_5"]
  op6["6: conv_6"]
  op7["7: depthwise_conv_7"]
  op8["8: conv_8"]
  op9["9: depthwise_conv_9"]
  op10["10: conv_10"]
  op11["11: depthwise_conv_11"]
  op12["12: conv_12"]
  op13["13: depthwise_conv_13"]
  op14["14: conv_14"]
  op15["15: depthwise_conv_15"]
  op16["16: conv_16"]
  op17["17: depthwise_conv_17"]
  op18["18: conv_18"]
  op19["19: depthwise_conv_19"]
  op20["20: conv_20"]
  op21["21: depthwise_conv_21"]
  op22["22: conv_22"]
  op23["23: depthwise_conv_23"]
  op24["24: conv_24"]
  op25["25: depthwise_conv_25"]
  op26["26: conv_26"]
  op27["27: average_pool_27"]
  op29["29: fully_connected_29"]
  op30["30: softmax_30"]
  op0 -->|"T59"| op1
  op1 -->|"T60"| op2
  op2 -->|"T61"| op3
  op3 -->|"T62"| op4
  op4 -->|"T63"| op5
  op5 -->|"T64"| op6
  op6 -->|"T65"| op7
  op7 -->|"T66"| op8
  op8 -->|"T67"| op9
  op9 -->|"T68"| op10
  op10 -->|"T69"| op11
  op11 -->|"T70"| op12
  op12 -->|"T71"| op13
  op13 -->|"T72"| op14
  op14 -->|"T73"| op15
  op15 -->|"T74"| op16
  op16 -->|"T75"| op17
  op17 -->|"T76"| op18
  op18 -->|"T77"| op19
  op19 -->|"T78"| op20
  op20 -->|"T79"| op21
  op21 -->|"T80"| op22
  op22 -->|"T81"| op23
  op23 -->|"T82"| op24
  op24 -->|"T83"| op25
  op25 -->|"T84"| op26
  op26 -->|"T85"| op27
  op27 -->|"T86"| op29
  op29 -->|"T88"| op30
```
